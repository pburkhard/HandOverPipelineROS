ros:
  node_name: "handover_pipeline"  # Name of the ROS node
  subscribed_topics:
    rgb_camera: "/camera/color/image_raw"  # Contains the rbgd image
    depth_camera: "/camera/aligned_depth_to_color/image_raw"  # Contains the depth image
    camera_info: "/camera/color/camera_info"  # Contains the camera info
    task: "/task"  # Contains information about the object and the task to be performed
    transform_selected_grasp_to_robot_cam: "/selected_grasp_marker"  # Contains the gripper pose in the robot camera frame
  published_topics:
    out_dir: "/out_dir"  # Contains the output directory path for saving results
    target_gripper_pose: "/target_gripper_pose"  # Contains the target gripper pose in the robot base frame
    marker: "/target_grasp_marker"  # Contains the marker for visualization
  frame_ids:
    base: "panda_link0"  # Frame ID for the robot base
    camera: "camera_color_optical_frame"  # Frame ID for the camera
    gripper: "selected_grasp"  # Frame ID for the gripper

grasp_generator_client:
  server_name: "grasp_generator"  # Name of the ROS service to call
  ros:
    timeout: 100  # Timeout for the service call in seconds

correspondence_estimator_client:
  server_name: "correspondence_estimator"  # Name of the ROS service to call
  ros:
    timeout: 100  # Timeout for the service call in seconds

hand_reconstructor_client:
  services:
    reconstruct_hand: "reconstruct_hand"  # Service to reconstruct the hand from a given image
    reconstruct_hand_pose: "reconstruct_hand_pose"  # Service to reconstruct the hand pose from a given image
    estimate_camera: "estimate_camera"  # Service to estimate the camera parameters
    render_hand: "render_hand"  # Service to render the reconstructed hand

transform_estimator_client:
  actions:
    estimate_transform: "estimate_transform"  # Service to estimate the transform between object and grasp image
    estimate_transform_heuristic: "estimate_transform_heuristic"  # Service to estimate the transform using a heuristic method
  ros:
    timeout: 100  # Timeout for the service call in seconds

# If true, run the correspondence estimator twice, once with the original object image and once with a mirrored version of the image.
# The one with the lower mean squared error at the transform estimation will be used.
try_mirrored_image: false

# Options for estimating the intrinsic matrix of the (virtual) gen camera.
# Options:
# - "grasp_image": Estimate from grasp image using the hand reconstructor. Attention: For this option to be available, the hand reconstructor must be enabled
# - "robot_camera" Use real camera intrinsics from the robot camera. Attention: For this option to be available, the camera subscriber must be enabled
# - "example_data": Load the camera intrinsics from the example data. This is as well the fallback option if the other ones cannot be used.
camera_info_source: "robot_camera"

# If true, the heuristic transform estimation will be used instead of solving the PnP problem. It is more robust, but less accurate.
use_heuristic_transform_estimation: false

# If true, the translation of the hand pose extracted by the hand reconstructor will be overwritten by the measured value from the camera depth image.
# This can help to stabilize the predicted pose. This setting only applies to the main loop, not to the initialization phase.
use_camera_depth_for_hand_pose: true

# Number of hand poses to keep in the history for smoothing the target hand pose.
hand_pose_history_size: 3

debug:
  # All output data will be saved in a folder in this base directory.
  out_dir: "/Handover_Pipeline/outputs/"
  # Directory from which example data will be loaded.
  example_dir: "/Handover_Pipeline/input/examples/02_hammer_real2"
  # If true, the configuration used for the pipeline will be logged.
  log_config: true
  # If true, intermediate results from the initialization phase will be saved to the output directory.
  log_init_results: true
  # If true, the target hand pose will be rendered in the object image for visualization..
  visualize_target_hand_pose: true
  # If true, the camera topic will not be subscribed to, and example data will be used instead.
  bypass_camera_subscriber: true
  # If true, the task topic will not be subscribed to, and example data will be used instead.
  bypass_task_subscriber: true
  # If true, the grasp generator will not be called, and example data will be used instead.
  bypass_grasp_generator: true
  # If true, the correspondence estimator will not be called, and example data will be used instead.
  bypass_correspondence_estimator: true
  # If true, the hand reconstructor will not be called, and example data will be used instead.
  bypass_hand_reconstructor: true
  # If true, the grasp -> object transform estimator will not be called, and example data will be used instead.
  bypass_transform_estimator: true
  # If true, the object -> gripper transform subscriber will not be used, and example data will be used instead.
  bypass_transform_subscriber: true
  # If true, the transform from selected grasp to hand pose will be loaded from a file and overwrites the one calculated during the initialization phase. 
  load_init_results_from_file: false
  # If true, the node will directly run the main loop without going through the initialization phase
  skip_initaization_phase: false
  # If true, the node will just run the initialization phase and exit.
  skip_main_loop: false
  # If true, the target pose is set equal to the hand pose, effectively ignoring the transform calculated during the initialization phase.
  use_hand_pose_as_target: false
  # If true, the target position is set to equal to the hand position, while the target orientation is calculated from the transform determined during the initialization phase.
  use_hand_position_as_target: false
  # If true, the main loop will publish a marker additional to the target transform for visualization.
  publish_marker: true
  # If true, verbose information will be logged during the main loop execution.
  log_verbose: true